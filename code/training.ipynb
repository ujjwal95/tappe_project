{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/aml/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import inflect\n",
    "import re\n",
    "import time\n",
    "# next we can import some sklearn libraries to start working with stuff\n",
    "## transformers and pipline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "## model selectors\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV, GroupKFold, GroupShuffleSplit\n",
    "## models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "## feature extractors\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from average_word_length_extractor import AverageWordLengthExtractor\n",
    "from question_extractor import QuestionExtractor\n",
    "from int_to_words_extractor import NumberStringExtractor\n",
    "from greeting_extractor import GreetingExtractor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from text_preprocessor import TextPreprocessor\n",
    "from sentiment_extractor import SentimentExtractor\n",
    "from ner_extractor import NERExtractor\n",
    "# save model\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train_files = glob.glob('../data/data_v1/TrainCSV_Updated/*.csv')\n",
    "train = pd.concat([pd.read_csv(fp).assign(train_set=re.split('_|, ',os.path.basename(fp))[0]) for fp in train_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stringList</th>\n",
       "      <th>speakerID</th>\n",
       "      <th>semanticType</th>\n",
       "      <th>leading</th>\n",
       "      <th>Symptom</th>\n",
       "      <th>PMH</th>\n",
       "      <th>MEDS</th>\n",
       "      <th>ALLG</th>\n",
       "      <th>FAMHx</th>\n",
       "      <th>lifestyle</th>\n",
       "      <th>...</th>\n",
       "      <th>GS4089</th>\n",
       "      <th>GS4090</th>\n",
       "      <th>GS4091</th>\n",
       "      <th>GS4092</th>\n",
       "      <th>GS4093</th>\n",
       "      <th>GS4094</th>\n",
       "      <th>GS4095</th>\n",
       "      <th>supportProvision</th>\n",
       "      <th>stringedQuestion</th>\n",
       "      <th>train_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>so why don't you tell me what brings you here ...</td>\n",
       "      <td>doctor</td>\n",
       "      <td>openQuestion</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070285</td>\n",
       "      <td>0.009144</td>\n",
       "      <td>-0.020626</td>\n",
       "      <td>0.031314</td>\n",
       "      <td>-0.003403</td>\n",
       "      <td>-0.006069</td>\n",
       "      <td>-0.008571</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>so I've been having this kind of random fast w...</td>\n",
       "      <td>patient</td>\n",
       "      <td>statement</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035766</td>\n",
       "      <td>-0.009635</td>\n",
       "      <td>0.036159</td>\n",
       "      <td>-0.019223</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.054135</td>\n",
       "      <td>0.056419</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>okay that's pretty fast</td>\n",
       "      <td>doctor</td>\n",
       "      <td>statement</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130121</td>\n",
       "      <td>-0.033890</td>\n",
       "      <td>0.023881</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.019991</td>\n",
       "      <td>0.093138</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 4142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          stringList speakerID  semanticType  \\\n",
       "0  so why don't you tell me what brings you here ...    doctor  openQuestion   \n",
       "1  so I've been having this kind of random fast w...   patient     statement   \n",
       "2                            okay that's pretty fast    doctor     statement   \n",
       "\n",
       "  leading Symptom PMH MEDS ALLG FAMHx lifestyle  ...    GS4089    GS4090  \\\n",
       "0      no      no  no   no   no    no        no  ...  0.070285  0.009144   \n",
       "1      no     yes  no   no   no    no        no  ...  0.035766 -0.009635   \n",
       "2      no      no  no   no   no    no        no  ...  0.130121 -0.033890   \n",
       "\n",
       "     GS4091    GS4092    GS4093    GS4094    GS4095 supportProvision  \\\n",
       "0 -0.020626  0.031314 -0.003403 -0.006069 -0.008571               no   \n",
       "1  0.036159 -0.019223  0.011710  0.054135  0.056419               no   \n",
       "2  0.023881  0.002090  0.000385  0.019991  0.093138               no   \n",
       "\n",
       "  stringedQuestion  train_set  \n",
       "0               no          3  \n",
       "1               no          3  \n",
       "2               no          3  \n",
       "\n",
       "[3 rows x 4142 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert yes to True(1) and no to False(0)\n",
    "train = train.replace(to_replace={'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove train_set columns\n",
    "train.drop(['train_set'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features over which we have to predict\n",
    "prediction_columns = ['stringedQuestion',\n",
    "                   'leading', 'Symptom', 'PMH', 'MEDS', 'ALLG', 'FAMHx', 'lifestyle',\n",
    "                   'pysch', 'SOCHx', 'sexualHistory', 'substanceUse', 'PE', 'FORM',\n",
    "                   'supportProvision', 'transition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract doctor interactions\n",
    "train = train.loc[train.speakerID == 'doctor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get case values\n",
    "cases = train['case_ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nas for now\n",
    "train.dropna(inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stringedQuestion: 2.496%\n",
      "leading: 0.263%\n",
      "Symptom: 20.297%\n",
      "PMH: 4.643%\n",
      "MEDS: 1.239%\n",
      "ALLG: 0.671%\n",
      "FAMHx: 3.735%\n",
      "lifestyle: 1.562%\n",
      "pysch: 2.216%\n",
      "SOCHx: 3.548%\n",
      "sexualHistory: 0.475%\n",
      "substanceUse: 2.496%\n",
      "PE: 13.531%\n",
      "FORM: 11.104%\n",
      "supportProvision: 3.311%\n",
      "transition: 8.124%\n"
     ]
    }
   ],
   "source": [
    "# check the balance for each prediction feature\n",
    "for n,i in enumerate(prediction_columns):\n",
    "    balance = train.loc[:, train.columns == prediction_columns[n]][i].value_counts()[1] / train.shape[0]\n",
    "    print('%s: %0.3f%%'%(i, balance*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "## count vectorizer\n",
    "### word level\n",
    "ngram_count_word = CountVectorizer(ngram_range=(1, 3), analyzer='word', token_pattern=r'\\w{1,}', max_features= 2000)\n",
    "### char level\n",
    "ngram_count_char = CountVectorizer(ngram_range=(1, 2), analyzer='char')\n",
    "## tf idf vectorizer\n",
    "### word level\n",
    "tf_idf_word = TfidfVectorizer(ngram_range=(1, 3), analyzer='word', token_pattern=r'\\w{1,}', max_features= 2000)\n",
    "### char level\n",
    "tf_idf_char = TfidfVectorizer(ngram_range=(1, 2), analyzer='char')\n",
    "text_preprocessor = TextPreprocessor()\n",
    "avg_word = AverageWordLengthExtractor()\n",
    "question_ex = QuestionExtractor()\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "numberstring= NumberStringExtractor()\n",
    "std_scaler = StandardScaler()\n",
    "greeting_ex = GreetingExtractor()\n",
    "sent_extractor = SentimentExtractor()\n",
    "ner_extractor = NERExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numbers to text\n",
    "num_to_str = ColumnTransformer([\n",
    "    ('numberstring', numberstring, 3)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# original feature encoders\n",
    "original_feat = ColumnTransformer([\n",
    "    ('avg_word', avg_word, 0),\n",
    "    ('question', question_ex, 0),\n",
    "    ('greeting', greeting_ex, 0),\n",
    "    ('sentiment', sent_extractor, 0),\n",
    "    ('ner', ner_extractor, 0)\n",
    "], remainder = 'passthrough')\n",
    "\n",
    "# text preprocessor\n",
    "text_pre = ColumnTransformer([\n",
    "    ('text_preprocessor', text_preprocessor, 0)\n",
    "], remainder= 'passthrough')\n",
    "\n",
    "# text encoders\n",
    "encoders = ColumnTransformer([\n",
    "    ('ngram_char', ngram_count_char, 0),\n",
    "    ('ngram_word', ngram_count_word, 0),\n",
    "    ('tdf_idf_char', tf_idf_char, 0),\n",
    "    ('tdf_idf_word', tf_idf_char, 0)\n",
    "], remainder = 'passthrough')\n",
    "\n",
    "# one hot encoding\n",
    "one_hot = ColumnTransformer([\n",
    "    ('one_hot', one_hot_encoder, ['semanticType'])\n",
    "], remainder = 'passthrough')\n",
    "\n",
    "# text pipeline\n",
    "text_features = Pipeline([\n",
    "    ('num_to_str', num_to_str),\n",
    "    ('original_feat', original_feat),\n",
    "    ('text_pre', text_pre),\n",
    "    ('encoders', encoders)\n",
    "])\n",
    "\n",
    "# preprocessing\n",
    "preprocess = Pipeline([\n",
    "    ('one_hot_encoder', one_hot),\n",
    "    ('text_feat', text_features),\n",
    "    ('std_scaler', std_scaler)\n",
    "])\n",
    "\n",
    "# ml pipeline\n",
    "ml_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocess),\n",
    "    ('model', LGBMClassifier(n_estimators = 100, n_jobs = -1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model dictionary\n",
    "models = {}\n",
    "# select columns by data\n",
    "X_train = train.drop(prediction_columns, axis=1)\n",
    "unusable_columns = ['speakerID', 'case_ID']\n",
    "X_train = X_train.drop(unusable_columns, axis=1)\n",
    "# shuffle split generator\n",
    "group_shuffle_split = GroupShuffleSplit(n_splits=1, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stringedQuestion:0.8014697734264332\n",
      "Time for variable stringedQuestion: 229.39206719398499sec\n",
      "leading:0.030889804276295313\n",
      "Time for variable leading: 266.3376178741455sec\n",
      "Symptom:0.9547048748032048\n",
      "Time for variable Symptom: 295.232079744339sec\n",
      "PMH:0.7885114023759484\n",
      "Time for variable PMH: 299.2815730571747sec\n",
      "MEDS:0.8615078717244583\n",
      "Time for variable MEDS: 233.33761620521545sec\n",
      "ALLG:1.0\n",
      "Time for variable ALLG: 228.71168899536133sec\n",
      "FAMHx:0.8320563896031345\n",
      "Time for variable FAMHx: 255.08805584907532sec\n",
      "lifestyle:0.8840681490477549\n",
      "Time for variable lifestyle: 253.30153703689575sec\n",
      "pysch:0.7780490757187416\n",
      "Time for variable pysch: 284.6124269962311sec\n",
      "SOCHx:0.7071571499405478\n",
      "Time for variable SOCHx: 264.4002182483673sec\n",
      "sexualHistory:0.7452115866589551\n",
      "Time for variable sexualHistory: 248.07816982269287sec\n",
      "substanceUse:0.9354087430257639\n",
      "Time for variable substanceUse: 252.40762186050415sec\n",
      "PE:0.9778911837393216\n",
      "Time for variable PE: 277.16190814971924sec\n",
      "FORM:0.8653104797926156\n",
      "Time for variable FORM: 262.910875082016sec\n",
      "supportProvision:0.7422611907851079\n",
      "Time for variable supportProvision: 306.55710315704346sec\n",
      "transition:0.7723805969770599\n",
      "Time for variable transition: 316.9436070919037sec\n"
     ]
    }
   ],
   "source": [
    "for n,i in enumerate(prediction_columns):\n",
    "    # select y\n",
    "    start = time.time()\n",
    "    y_train = train.loc[:, train.columns == prediction_columns[n]]\n",
    "    gen = group_shuffle_split.split(X_train,y_train,cases)\n",
    "    \n",
    "    print(i + ':' + str(cross_val_score(ml_pipeline, X_train, y_train, scoring = 'average_precision', cv = gen).mean()))\n",
    "    print('Time for variable ' + i + ': ' + str(time.time()-start) + 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.849):\n",
      "Time for variable stringedQuestion: 3334.3416290283203 sec\n",
      "Best parameter (CV score=0.286):\n",
      "Time for variable leading: 3373.469489097595 sec\n",
      "Best parameter (CV score=0.943):\n",
      "Time for variable Symptom: 3757.107565164566 sec\n",
      "Best parameter (CV score=0.769):\n",
      "Time for variable PMH: 3517.8814380168915 sec\n",
      "Best parameter (CV score=0.924):\n",
      "Time for variable MEDS: 19912.929650068283 sec\n",
      "Best parameter (CV score=1.000):\n",
      "Time for variable ALLG: 3261.4875032901764 sec\n",
      "Best parameter (CV score=0.894):\n",
      "Time for variable FAMHx: 3195.3665862083435 sec\n",
      "Best parameter (CV score=0.920):\n",
      "Time for variable lifestyle: 3125.5118799209595 sec\n",
      "Best parameter (CV score=0.785):\n",
      "Time for variable pysch: 3242.5578072071075 sec\n",
      "Best parameter (CV score=0.693):\n",
      "Time for variable SOCHx: 3269.94765996933 sec\n",
      "Best parameter (CV score=0.691):\n",
      "Time for variable sexualHistory: 3021.43749499321 sec\n",
      "Best parameter (CV score=0.895):\n",
      "Time for variable substanceUse: 3012.010580062866 sec\n",
      "Best parameter (CV score=0.956):\n",
      "Time for variable PE: 4212.386872053146 sec\n",
      "Best parameter (CV score=0.893):\n",
      "Time for variable FORM: 3484.1330580711365 sec\n",
      "Best parameter (CV score=0.808):\n",
      "Time for variable supportProvision: 6495.383873939514 sec\n",
      "Best parameter (CV score=0.859):\n",
      "Time for variable transition: 3542.7361948490143 sec\n"
     ]
    }
   ],
   "source": [
    "grid = {\n",
    "    'model__n_estimators': np.logspace(1, 3.5, 6).astype(int),\n",
    "    'model__max_depth': np.linspace(2, 10, 5).astype(int),\n",
    "    'model__num_leaves': np.linspace(4, 100, 5).astype(int),\n",
    "    'model__boosting_type': ['gbdt', 'dart'],\n",
    "    'model__learning_rate': np.logspace(-3, 1, 9)\n",
    "}\n",
    "\n",
    "for n, i in enumerate(prediction_columns):\n",
    "    # select y\n",
    "    start = time.time()\n",
    "    y_train = train.loc[:, train.columns == prediction_columns[n]]\n",
    "    gen = group_shuffle_split.split(X_train,y_train,cases)\n",
    "    \n",
    "    grid_search = RandomizedSearchCV(ml_pipeline, param_distributions=grid, n_iter = 10, scoring = 'average_precision', cv = gen, random_state= 42)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "    best_pipeline = grid_search.best_estimator_\n",
    "    models[i] = best_pipeline\n",
    "    print('Time for variable ' + i + ': ' + str(time.time()-start) + ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to directory\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "directory = '../model/' + timestr + '/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/20190427-134642/LightGBM-grid-search-20190427-134642.pkl']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model in folder\n",
    "file_name = 'LightGBM-grid-search-' + timestr + '.pkl'\n",
    "joblib.dump(models, directory + file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
